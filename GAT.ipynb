{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799092ae",
   "metadata": {},
   "source": [
    "# Graph Attention Networks\n",
    "\n",
    "Graph Attention Network, or GAT for short, is a Graph Neural Network (GNN). In this notebook we will see a the implementation of the model along with training and accuracy\n",
    "\n",
    "## 1 Importing the Libraries\n",
    "We will import the required libraries to build the model\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html): The pickle module implements binary protocols for serializing and de-serializing a Python object structure. This is used to load the data\n",
    "* [scipy](https://docs.scipy.org/doc/scipy/reference/sparse.html): It provides functions to deal with sparse data\n",
    "* [numpy](https://numpy.org/): It is a library consisting of multidimensional array objects and a collection of routines for processing of array\n",
    "* [torch](https://pytorch.org/): PyTorch is a Python package that provides two high-level features. Tensor computation (like NumPy) with strong GPU acceleration and Deep neural networks built on a tape-based autograd system\n",
    "* [os](https://docs.python.org/3/library/os.html): It is used for operating system dependent functionality.\n",
    "* [enum](https://docs.python.org/3/library/enum.html) : It is used to implement Enumerations\n",
    "* [git](https://gitpython.readthedocs.io/en/stable/intro.html#):  is a python library used to interact with git repositories\n",
    "* [re](https://docs.python.org/3/library/re.html): This module provides regular expression matching operations\n",
    "* [argparse](https://docs.python.org/3/library/argparse.html): The module makes it easy to write user-friendly command-line interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88160beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Loading data\n",
    "import pickle\n",
    "\n",
    "# Main computation libraries\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning related imports\n",
    "import torch\n",
    "\n",
    "# To define constants and access directories\n",
    "import os\n",
    "import enum\n",
    "\n",
    "# To implement GAT inner workings\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "import git # To store the checkpoint\n",
    "import re  # regex\n",
    "\n",
    "\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f807d3",
   "metadata": {},
   "source": [
    "## 2 Constants for the GAT Model\n",
    "\n",
    "Here we will define the constants that we will use in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55ccc9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(enum.Enum):\n",
    "    CORA = 0\n",
    "\n",
    "# We'll be dumping and reading the data from this directory\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')\n",
    "CORA_PATH = os.path.join(DATA_DIR_PATH, 'cora')  # this is checked-in no need to make a directory\n",
    "    \n",
    "CORA_TRAIN_RANGE = [0, 140]  # we're using the first 140 nodes as the training nodes\n",
    "CORA_VAL_RANGE = [140, 140+500]\n",
    "CORA_TEST_RANGE = [1708, 1708+1000]\n",
    "CORA_NUM_INPUT_FEATURES = 1433\n",
    "CORA_NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff55c4e",
   "metadata": {},
   "source": [
    "## 2 Helper Functions\n",
    "\n",
    "This section will contain a bunch of helper functions that will be used to build the model\n",
    "\n",
    "### 2.1 Loading and Saving Data\n",
    "First let's define these simple functions for loading/saving Pickle files - we need them for Cora. All Cora data is stored as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f28d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_read(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def pickle_save(path, data):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb628749",
   "metadata": {},
   "source": [
    "### 2.2 Normalizing the features\n",
    "\n",
    "This function takes the node features as input and normalises them. We do multiplication with inverse sum of features.\n",
    "* **Step 1** : We first Calculate sum features for every node feature vector. This has a shape (N, FIN) -> (N, 1), where N number of nodes and FIN number of input features\n",
    "* **Step 2** : We make it inverse because * by 1/x is better (faster) then / by x. This has a shape = (N, 1) -> (N)\n",
    "* **Step 3** : Again certain sums will be 0 so 1/0 will give us inf so we replace those by 1 which is a neutral element for mul\n",
    "* **Step 4** : Create a diagonal matrix whose values on the diagonal come from node_features_inv_sum\n",
    "* **Step 5** : We return the normalized features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1aff377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features_sparse(node_features_sparse):\n",
    "    assert sp.issparse(node_features_sparse), f'Expected a sparse matrix, got {node_features_sparse}.'\n",
    "\n",
    "    node_features_sum = np.array(node_features_sparse.sum(-1))  # Step 1\n",
    "\n",
    "    node_features_inv_sum = np.power(node_features_sum, -1).squeeze() # Step 2\n",
    "\n",
    "    node_features_inv_sum[np.isinf(node_features_inv_sum)] = 1. # Step 3\n",
    "\n",
    "    diagonal_inv_features_sum_matrix = sp.diags(node_features_inv_sum) # Step 4\n",
    "\n",
    "    return diagonal_inv_features_sum_matrix.dot(node_features_sparse) # Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1855561",
   "metadata": {},
   "source": [
    "### 2.3 Building Edge Index\n",
    "This function build the edge index for each of the edges in the graph\n",
    "* **Step 1** : It iterates through all the nodes\n",
    "* **Step 2** : For each neighbour we check if we have visited that edge\n",
    "* **Step 3** : We assign an edge index to a non visited edge\n",
    "* **Step 4** : We stack the edge index and the nodes. This is of shape = (2, E), where E is the number of edges in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6245c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True):\n",
    "    source_nodes_ids, target_nodes_ids = [], []\n",
    "    seen_edges = set()\n",
    "\n",
    "    for src_node, neighboring_nodes in adjacency_list_dict.items(): # Step 1\n",
    "        for trg_node in neighboring_nodes: # Step 2\n",
    "            if (src_node, trg_node) not in seen_edges:  \n",
    "                source_nodes_ids.append(src_node)\n",
    "                target_nodes_ids.append(trg_node)\n",
    "\n",
    "                seen_edges.add((src_node, trg_node)) # Step 3\n",
    "\n",
    "    if add_self_edges:\n",
    "        source_nodes_ids.extend(np.arange(num_of_nodes))\n",
    "        target_nodes_ids.extend(np.arange(num_of_nodes))\n",
    "\n",
    "    edge_index = np.row_stack((source_nodes_ids, target_nodes_ids)) # Step 4\n",
    "\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81a984",
   "metadata": {},
   "source": [
    "### 2.4 Loading Graph Data\n",
    "This function creates the graph Data from the Cora Dataset. The predefined datasets include\n",
    "1. node_features.csr - This contains the feautes of the node. The shape of is (N, FIN), where N is the number of nodes and FIN is the number of input features\n",
    "2. node_labels.npy - This contains the labels of the respective nodes. The shape of this is (N, 1), where N is the number of nodes \n",
    "3. adjacency_list.dict - This defins the nodes and the corresponding neighbours. The shape of this is (N, number of neighboring nodes). This is a dictionary not a matrix.\n",
    "\n",
    "* **Step 1** : We normalize the data. This helps with training\n",
    "* **Step 2** : We get edge index based on the graph. This is of shape = (2, E), where E is the number of edges, and 2 for source and target nodes. Basically edge index contains tuples of the format S->T, e.g. 0->3 means that node with id 0 points to a node with id 3.\n",
    "* **Step 3** : Convert to dense PyTorch tensors. Needs to be long int type because later functions like PyTorch's index_select expect it\n",
    "* **Step 4** : Get Indices that help us extract nodes that belong to the train/val and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79acf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(training_config, device):\n",
    "    dataset_name = training_config['dataset_name'].lower()\n",
    "\n",
    "    if dataset_name == DatasetType.CORA.name.lower():\n",
    "\n",
    "        \n",
    "        node_features_csr = pickle_read(os.path.join(CORA_PATH, 'node_features.csr'))\n",
    "\n",
    "        node_labels_npy = pickle_read(os.path.join(CORA_PATH, 'node_labels.npy'))\n",
    "\n",
    "        adjacency_list_dict = pickle_read(os.path.join(CORA_PATH, 'adjacency_list.dict'))\n",
    "\n",
    "        node_features_csr = normalize_features_sparse(node_features_csr) # Step 1\n",
    "        num_of_nodes = len(node_labels_npy)\n",
    "\n",
    "        topology = build_edge_index(adjacency_list_dict, num_of_nodes, add_self_edges=True) # Step 2\n",
    "\n",
    "        \n",
    "        # Step 3\n",
    "        topology = torch.tensor(topology, dtype=torch.long, device=device)\n",
    "        node_labels = torch.tensor(node_labels_npy, dtype=torch.long, device=device)  \n",
    "        node_features = torch.tensor(node_features_csr.todense(), device=device)\n",
    "\n",
    "        # Step 4\n",
    "        train_indices = torch.arange(CORA_TRAIN_RANGE[0], CORA_TRAIN_RANGE[1], dtype=torch.long, device=device)\n",
    "        val_indices = torch.arange(CORA_VAL_RANGE[0], CORA_VAL_RANGE[1], dtype=torch.long, device=device)\n",
    "        test_indices = torch.arange(CORA_TEST_RANGE[0], CORA_TEST_RANGE[1], dtype=torch.long, device=device)\n",
    "\n",
    "        return node_features, node_labels, topology, train_indices, val_indices, test_indices\n",
    "    else:\n",
    "        raise Exception(f'{dataset_name} not yet supported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb93de",
   "metadata": {},
   "source": [
    "### 2.5 Final Data Precossing\n",
    "\n",
    "We now finaly create the data that will be used to train the model\n",
    "* **Step 1** : Checking whether you have a GPU\n",
    "* **Step 2** : Loading Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2326b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433]) torch.float32\n",
      "torch.Size([2708]) torch.int64\n",
      "torch.Size([2, 13264]) torch.int64\n",
      "torch.Size([140]) torch.int64\n",
      "torch.Size([500]) torch.int64\n",
      "torch.Size([1000]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "config = {\n",
    "    'dataset_name': 'CORA',\n",
    "}\n",
    "\n",
    "node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
    "\n",
    "print(node_features.shape, node_features.dtype)\n",
    "print(node_labels.shape, node_labels.dtype)\n",
    "print(edge_index.shape, edge_index.dtype)\n",
    "print(train_indices.shape, train_indices.dtype)\n",
    "print(val_indices.shape, val_indices.dtype)\n",
    "print(test_indices.shape, test_indices.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20dcbf1",
   "metadata": {},
   "source": [
    "## 3 GAT Inner Workings\n",
    "\n",
    "This section explains the inner workings of the GAT Model\n",
    "\n",
    "### 3.1 GAT Layer\n",
    "We will define functions that are required by a single layer in this sections\n",
    "\n",
    "### 3.1.1 Initialisation\n",
    "\n",
    "* **Step 1** : Initiliase constants that will be used in the model\n",
    "* **Step 2** : Create the trainable weights linear projection matrix (denoted as \"W\" in the paper), attention target/source(denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo).\n",
    "* **Step 3** : After we concatenate target node (node i) and source node (node j) we apply the \"additive\" scoring function which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same. Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\" we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up.\n",
    "* **Step 4** : Initialise the Bias if nessecary\n",
    "* **Step 5** : Initialise the Add Slip connection if necessary\n",
    "* **Step 6** : Initialise the Leaky ReLu, using 0.2 as in the paper\n",
    "\n",
    "### 3.1.2 Forward Implementation\n",
    "* **Step 1** : Linear Projection + regularization\n",
    "    * **Step 1.1** We apply the dropout to all of the input node features.\n",
    "    * **Step 1.2** We project the input node features into NH independent output features (one for each attention head).\n",
    "    \n",
    "* **Step 2** : Edge attention calculation\n",
    "    * **Step 2.1** Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "    * **Step 2.2** We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all the possible combinations of scores we &emsp;just prepare those that will actually be used and those are defined by the edge index.\n",
    "    * **Step 2.3** Add stochasticity to neighborhood aggregation\n",
    "\n",
    "* **Step 3** : Neighborhood aggregation\n",
    "    * **Step 3.1** Element-wise (aka Hadamard) product\n",
    "    * **Step 3.2** Sum up weighted and projected neighborhood feature vectors for every target node\n",
    "\n",
    "* **Step 4** : Residual/skip connections, concat and bias\n",
    "\n",
    "### 3.1.2 Neighbourhood Aware Softmax\n",
    " As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph. Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3 in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3) (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3 i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.\n",
    " \n",
    "* **Step 1** : Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)\n",
    "* **Step 2** : Calculate the denominator. shape = (E, NH)<\n",
    "* **Step 3** : 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the possibility of the computer rounding a very small number all the way to 0.\n",
    "* **Step 4** : reshape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6d18230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(torch.nn.Module):\n",
    "    \n",
    "    # We'll use these constants in many functions \n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of \"N\" in tensor)\n",
    "    head_dim = 1       # attention head dim\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),\n",
    "                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Step 1\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        self.concat = concat  # whether we should concatenate or average the attention heads\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "\n",
    "        \n",
    "        # Step 2\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # Step 3\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        # Step 4\n",
    "        if bias and concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))\n",
    "        elif bias and not concat:\n",
    "            self.bias = nn.Parameter(torch.Tensor(num_out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "        # Step 5\n",
    "        if add_skip_connection:\n",
    "            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "        else:\n",
    "            self.register_parameter('skip_proj', None)\n",
    "\n",
    "        #\n",
    "        # End of trainable weights\n",
    "        #\n",
    "\n",
    "        self.leakyReLU = nn.LeakyReLU(0.2)  # Step 3.1.\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "       \n",
    "        self.init_params()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features, edge_index = data  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'\n",
    "        \n",
    "        # Step 1.1\n",
    "        in_nodes_features = self.dropout(in_nodes_features)\n",
    "\n",
    "        # Step 1.2\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "        \n",
    "        # Step 2.1\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        \n",
    "        # Step 2.2\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)\n",
    "\n",
    "        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)\n",
    "        \n",
    "        # Step 2.3\n",
    "        attentions_per_edge = self.dropout(attentions_per_edge)\n",
    "\n",
    "        #\n",
    "        # Step 3: Neighborhood aggregation\n",
    "        #\n",
    "\n",
    "        # Step 3.1\n",
    "        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge\n",
    "\n",
    "        # Step 3.2\n",
    "        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)\n",
    "\n",
    "        #\n",
    "        # Step 4: Residual/skip connections, concat and bias\n",
    "        #\n",
    "\n",
    "        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)\n",
    "        return (out_nodes_features, edge_index)\n",
    "\n",
    "    #\n",
    "    # Helper functions (without comments there is very little code so don't be scared!)\n",
    "    #\n",
    "\n",
    "    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):\n",
    "        \n",
    "        # Step 1\n",
    "        scores_per_edge = scores_per_edge - scores_per_edge.max()\n",
    "        exp_scores_per_edge = scores_per_edge.exp()  # softmax\n",
    "\n",
    "        # Step 2\n",
    "        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)\n",
    "\n",
    "        # Step 3\n",
    "        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)\n",
    "\n",
    "        # Step 4\n",
    "        return attentions_per_edge.unsqueeze(-1)\n",
    "\n",
    "    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):\n",
    "        \n",
    "        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)\n",
    "\n",
    "        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads\n",
    "        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes\n",
    "        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)\n",
    "\n",
    "        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the\n",
    "        # target index)\n",
    "        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)\n",
    "\n",
    "        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to\n",
    "        # all the locations where the source nodes pointed to i (as dictated by the target index)\n",
    "        # shape = (N, NH) -> (E, NH)\n",
    "        return neighborhood_sums.index_select(self.nodes_dim, trg_index)\n",
    "\n",
    "    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):\n",
    "        \n",
    "        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible\n",
    "        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)\n",
    "        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)\n",
    "\n",
    "        # shape = (E) -> (E, NH, FOUT)\n",
    "        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)\n",
    "        # aggregation step - we accumulate projected, weighted node features for all the attention heads\n",
    "        # shape = (E, NH, FOUT) -> (N, NH, FOUT)\n",
    "        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)\n",
    "\n",
    "        return out_nodes_features\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "    def explicit_broadcast(self, this, other):\n",
    "        # Append singleton dimensions until this.dim() == other.dim()\n",
    "        for _ in range(this.dim(), other.dim()):\n",
    "            this = this.unsqueeze(-1)\n",
    "\n",
    "        # Explicitly expand so that shapes are the same\n",
    "        return this.expand_as(other)\n",
    "\n",
    "    def init_params(self):\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):\n",
    "\n",
    "        if self.add_skip_connection:  # add skip or residual connection\n",
    "            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT\n",
    "                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH\n",
    "                # thus we're basically copying input vectors NH times and adding to processed vectors\n",
    "                out_nodes_features += in_nodes_features.unsqueeze(1)\n",
    "            else:\n",
    "                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output\n",
    "                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.\n",
    "                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        if self.concat:\n",
    "            # shape = (N, NH, FOUT) -> (N, NH*FOUT)\n",
    "            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)\n",
    "        else:\n",
    "            # shape = (N, NH, FOUT) -> (N, FOUT)\n",
    "            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out_nodes_features += self.bias\n",
    "\n",
    "        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77ab48",
   "metadata": {},
   "source": [
    "### 3.2 GAT Layer\n",
    "This combines all the layers to form a multilayer GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96447129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,\n",
    "                 dropout=0.6):\n",
    "        super().__init__()\n",
    "        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'\n",
    "\n",
    "        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below\n",
    "\n",
    "        gat_layers = []  # collect GAT layers\n",
    "        for i in range(num_of_layers):\n",
    "            layer = GATLayer(\n",
    "                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation\n",
    "                num_out_features=num_features_per_layer[i+1],\n",
    "                num_of_heads=num_heads_per_layer[i+1],\n",
    "                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat\n",
    "                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores\n",
    "                dropout_prob=dropout,\n",
    "                add_skip_connection=add_skip_connection,\n",
    "                bias=bias\n",
    "            )\n",
    "            gat_layers.append(layer)\n",
    "\n",
    "        self.gat_net = nn.Sequential(\n",
    "            *gat_layers,\n",
    "        )\n",
    "\n",
    "    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:\n",
    "    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698\n",
    "    def forward(self, data):\n",
    "        return self.gat_net(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ec415",
   "metadata": {},
   "source": [
    "## 4 Training GAT\n",
    "We will finally start training the GAT\n",
    "\n",
    "### 4.1 Initialisation\n",
    "* **Step 1** : We will define three different model phases to depict training, validation and testing.\n",
    "* **Step 2** : We will define Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any improvement on the validation dataset (measured via accuracy metric), we'll break out from the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56aa44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "class LoopPhase(enum.Enum):\n",
    "    TRAIN = 0,\n",
    "    VAL = 1,\n",
    "    TEST = 2\n",
    "\n",
    "# Step 2\n",
    "BEST_VAL_ACC = 0\n",
    "BEST_VAL_LOSS = 0\n",
    "PATIENCE_CNT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf75b5",
   "metadata": {},
   "source": [
    "### 4.2 Get Training State\n",
    "\n",
    "This function gets the model parameter and required implementation to print various stages of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db260ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_state(training_config, model):\n",
    "    training_state = {\n",
    "        # Training details\n",
    "        \"dataset_name\": training_config['dataset_name'],\n",
    "        \"num_of_epochs\": training_config['num_of_epochs'],\n",
    "        \"test_acc\": training_config['test_acc'],\n",
    "\n",
    "        # Model structure\n",
    "        \"num_of_layers\": training_config['num_of_layers'],\n",
    "        \"num_heads_per_layer\": training_config['num_heads_per_layer'],\n",
    "        \"num_features_per_layer\": training_config['num_features_per_layer'],\n",
    "        \"add_skip_connection\": training_config['add_skip_connection'],\n",
    "        \"bias\": training_config['bias'],\n",
    "        \"dropout\": training_config['dropout'],\n",
    "\n",
    "        # Model state\n",
    "        \"state_dict\": model.state_dict()\n",
    "    }\n",
    "\n",
    "    return training_state\n",
    "\n",
    "\n",
    "def print_model_metadata(training_state):\n",
    "    header = f'\\n{\"*\"*5} Model training metadata: {\"*\"*5}'\n",
    "    print(header)\n",
    "\n",
    "    for key, value in training_state.items():\n",
    "        if key != 'state_dict':  # don't print state_dict just a bunch of numbers...\n",
    "            print(f'{key}: {value}')\n",
    "    print(f'{\"*\" * len(header)}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7e03b",
   "metadata": {},
   "source": [
    "### 4.3 Get Training Args\n",
    "\n",
    "This function creates the various arguments that will contain the model parameters and other information. It finally returns the training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eda5ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_training_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Training related\n",
    "    parser.add_argument(\"--num_of_epochs\", type=int, help=\"number of training epochs\", default=10000)\n",
    "    parser.add_argument(\"--patience_period\", type=int, help=\"number of epochs with no improvement on val before terminating\", default=1000)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"model learning rate\", default=5e-3)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, help=\"L2 regularization on model weights\", default=5e-4)\n",
    "    parser.add_argument(\"--should_test\", type=bool, help='should test the model on the test dataset?', default=True)\n",
    "\n",
    "    # Dataset related\n",
    "    parser.add_argument(\"--dataset_name\", choices=[el.name for el in DatasetType], help='dataset to use for training', default=DatasetType.CORA.name)\n",
    "    parser.add_argument(\"--should_visualize\", type=bool, help='should visualize the dataset?', default=False)\n",
    "\n",
    "    # Logging/debugging/checkpoint related (helps a lot with experimentation)\n",
    "    parser.add_argument(\"--enable_tensorboard\", type=bool, help=\"enable tensorboard logging\", default=False)\n",
    "    parser.add_argument(\"--console_log_freq\", type=int, help=\"log to output console (epoch) freq (None for no logging)\", default=100)\n",
    "    parser.add_argument(\"--checkpoint_freq\", type=int, help=\"checkpoint model saving (epoch) freq (None for no logging)\", default=1000)\n",
    "    args = parser.parse_args(\"\")\n",
    "\n",
    "    # Model architecture related - this is the architecture as defined in the official paper (for Cora classification)\n",
    "    gat_config = {\n",
    "        \"num_of_layers\": 2,  # GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)\n",
    "        \"num_heads_per_layer\": [8, 1],\n",
    "        \"num_features_per_layer\": [CORA_NUM_INPUT_FEATURES, 8, CORA_NUM_CLASSES],\n",
    "        \"add_skip_connection\": False,  # hurts perf on Cora\n",
    "        \"bias\": True,  # result is not so sensitive to bias\n",
    "        \"dropout\": 0.6,  # result is sensitive to dropout\n",
    "    }\n",
    "\n",
    "    # Wrapping training configuration into a dictionary\n",
    "    training_config = dict()\n",
    "    for arg in vars(args):\n",
    "        training_config[arg] = getattr(args, arg)\n",
    "\n",
    "    # Add additional config information\n",
    "    training_config.update(gat_config)\n",
    "\n",
    "    return training_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe866ef",
   "metadata": {},
   "source": [
    "### 4.4 Main Loop\n",
    "\n",
    "We will now define a simple decorator function so that we don't have to pass arguments that don't change from epoch to epoch\n",
    "\n",
    "* **Step 1** : Certain modules behave differently depending on whether we're training the model or not. e.g. nn.Dropout - we only want to drop model weights during the training.\n",
    "* **Step 2** : Do a forwards pass and extract only the relevant node scores (train/val or test ones). Note: [0] just extracts the node_features part of the data (index 1 contains the edge_index)\n",
    "* **Step 3** : Calculater the cross entropy. Example: let's take an output for a single node on Cora - it's a vector of size 7 and it contains unnormalized scores like: V = [-1.393,  3.0765, -2.4445,  9.6219,  2.1658, -5.5243, -4.6247] What PyTorch's cross entropy loss does is for every such vector it first applies a softmax, and so we'll have the V transformed into: [1.6421e-05, 1.4338e-03, 5.7378e-06, 0.99797, 5.7673e-04, 2.6376e-07, 6.4848e-07] Secondly, whatever the correct class is (say it's 3), it will then take the element at position 3, 0.99797 in this case, and the loss will be -log(0.99797). It does this for every node and applies a mean. You can see that as the probability of the correct class for most nodes approaches 1 we get to 0 loss!\n",
    "* **Step 4** : If the model is training, do the dradient descent.\n",
    "* **Step 5** : Accuracy metric. Finds the index of maximum (unnormalized) score for every node and that's the class prediction for that node. Compare those to true (ground truth) labels and find the fraction of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4410be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_loop(config, gat, cross_entropy_loss, optimizer, node_features, node_labels, edge_index, train_indices, val_indices, test_indices, patience_period, time_start):\n",
    "\n",
    "    node_dim = 0  \n",
    "\n",
    "    train_labels = node_labels.index_select(node_dim, train_indices)\n",
    "    val_labels = node_labels.index_select(node_dim, val_indices)\n",
    "    test_labels = node_labels.index_select(node_dim, test_indices)\n",
    "\n",
    "    # node_features shape = (N, FIN), edge_index shape = (2, E)\n",
    "    graph_data = (node_features, edge_index)  # I pack data into tuples because GAT uses nn.Sequential which requires it\n",
    "\n",
    "    def get_node_indices(phase):\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            return train_indices\n",
    "        elif phase == LoopPhase.VAL:\n",
    "            return val_indices\n",
    "        else:\n",
    "            return test_indices\n",
    "\n",
    "    def get_node_labels(phase):\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            return train_labels\n",
    "        elif phase == LoopPhase.VAL:\n",
    "            return val_labels\n",
    "        else:\n",
    "            return test_labels\n",
    "\n",
    "    def main_loop(phase, epoch=0):\n",
    "        global BEST_VAL_ACC, BEST_VAL_LOSS, PATIENCE_CNT\n",
    "\n",
    "        # Step 1\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            gat.train()\n",
    "        else:\n",
    "            gat.eval()\n",
    "\n",
    "        node_indices = get_node_indices(phase)\n",
    "        gt_node_labels = get_node_labels(phase)  # gt stands for ground truth\n",
    "\n",
    "        # Step 2\n",
    "        nodes_unnormalized_scores = gat(graph_data)[0].index_select(node_dim, node_indices)\n",
    "\n",
    "        # Step 3\n",
    "        loss = cross_entropy_loss(nodes_unnormalized_scores, gt_node_labels)\n",
    "\n",
    "        # Step 4\n",
    "        if phase == LoopPhase.TRAIN:\n",
    "            optimizer.zero_grad()  # clean the trainable weights gradients in the computational graph (.grad fields)\n",
    "            loss.backward()  # compute the gradients for every trainable weight in the computational graph\n",
    "            optimizer.step()  # apply the gradients to weights\n",
    "\n",
    "        # Step 5\n",
    "        class_predictions = torch.argmax(nodes_unnormalized_scores, dim=-1)\n",
    "        accuracy = torch.sum(torch.eq(class_predictions, gt_node_labels).long()).item() / len(gt_node_labels)\n",
    "\n",
    "        #\n",
    "        # Logging\n",
    "        #\n",
    "\n",
    "        \n",
    "\n",
    "        if phase == LoopPhase.VAL:\n",
    "\n",
    "            # Log to console\n",
    "            if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0:\n",
    "                print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] | epoch={epoch + 1} | val acc={accuracy}')\n",
    "\n",
    "            # The \"patience\" logic - should we break out from the training loop? If either validation acc keeps going up\n",
    "            # or the val loss keeps going down we won't stop\n",
    "            if accuracy > BEST_VAL_ACC or loss.item() < BEST_VAL_LOSS:\n",
    "                BEST_VAL_ACC = max(accuracy, BEST_VAL_ACC)  # keep track of the best validation accuracy so far\n",
    "                BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)\n",
    "                PATIENCE_CNT = 0  # reset the counter every time we encounter new best accuracy\n",
    "            else:\n",
    "                PATIENCE_CNT += 1  # otherwise keep counting\n",
    "\n",
    "            if PATIENCE_CNT >= patience_period:\n",
    "                raise Exception('Stopping the training')\n",
    "\n",
    "        else:\n",
    "            return accuracy  # in the case of test phase we just report back the test accuracy\n",
    "\n",
    "    return main_loop  # return the decorated function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aea7a8",
   "metadata": {},
   "source": [
    "### 4.5 Final Training the GAT\n",
    "* **Step 1** : Load the graph data\n",
    "* **Step 2** : Prepare the model\n",
    "* **Step 3** : Prepare other training related utilities (loss & optimizer and decorator function)\n",
    "* **Step 4** : Start the training procedure\n",
    "* **Step 5** : Potentially test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bb6f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_gat(config):\n",
    "    global BEST_VAL_ACC, BEST_VAL_LOSS\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # checking whether you have a GPU, I hope so!\n",
    "\n",
    "    # Step 1\n",
    "    node_features, node_labels, edge_index, train_indices, val_indices, test_indices = load_graph_data(config, device)\n",
    "\n",
    "    # Step 2\n",
    "    gat = GAT(\n",
    "        num_of_layers=config['num_of_layers'],\n",
    "        num_heads_per_layer=config['num_heads_per_layer'],\n",
    "        num_features_per_layer=config['num_features_per_layer'],\n",
    "        add_skip_connection=config['add_skip_connection'],\n",
    "        bias=config['bias'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Step 3\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    \n",
    "    main_loop = get_main_loop(\n",
    "        config,\n",
    "        gat,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        node_features,\n",
    "        node_labels,\n",
    "        edge_index,\n",
    "        train_indices,\n",
    "        val_indices,\n",
    "        test_indices,\n",
    "        config['patience_period'],\n",
    "        time.time())\n",
    "\n",
    "    BEST_VAL_ACC, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]  # reset vars used for early stopping\n",
    "\n",
    "    # Step 4\n",
    "    for epoch in range(config['num_of_epochs']):\n",
    "        # Training loop\n",
    "        main_loop(phase=LoopPhase.TRAIN, epoch=epoch)\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                main_loop(phase=LoopPhase.VAL, epoch=epoch)\n",
    "            except Exception as e:  # \"patience has run out\" exception :O\n",
    "                print(str(e))\n",
    "                break  # break out from the training loop\n",
    "\n",
    "    # Step 5\n",
    "    if config['should_test']:\n",
    "        test_acc = main_loop(phase=LoopPhase.TEST)\n",
    "        config['test_acc'] = test_acc\n",
    "        print(f'Test accuracy = {test_acc}')\n",
    "    else:\n",
    "        config['test_acc'] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d17c7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT training: time elapsed= 0.10 [s] | epoch=1 | val acc=0.302\n",
      "GAT training: time elapsed= 8.21 [s] | epoch=101 | val acc=0.756\n",
      "GAT training: time elapsed= 16.38 [s] | epoch=201 | val acc=0.784\n",
      "GAT training: time elapsed= 24.46 [s] | epoch=301 | val acc=0.806\n",
      "GAT training: time elapsed= 32.44 [s] | epoch=401 | val acc=0.812\n",
      "GAT training: time elapsed= 40.49 [s] | epoch=501 | val acc=0.804\n",
      "GAT training: time elapsed= 48.97 [s] | epoch=601 | val acc=0.79\n",
      "GAT training: time elapsed= 57.36 [s] | epoch=701 | val acc=0.794\n",
      "GAT training: time elapsed= 65.75 [s] | epoch=801 | val acc=0.806\n",
      "GAT training: time elapsed= 74.09 [s] | epoch=901 | val acc=0.814\n",
      "GAT training: time elapsed= 82.20 [s] | epoch=1001 | val acc=0.796\n",
      "GAT training: time elapsed= 90.19 [s] | epoch=1101 | val acc=0.794\n",
      "GAT training: time elapsed= 98.19 [s] | epoch=1201 | val acc=0.808\n",
      "GAT training: time elapsed= 106.38 [s] | epoch=1301 | val acc=0.808\n",
      "GAT training: time elapsed= 114.72 [s] | epoch=1401 | val acc=0.808\n",
      "GAT training: time elapsed= 123.05 [s] | epoch=1501 | val acc=0.804\n",
      "GAT training: time elapsed= 131.25 [s] | epoch=1601 | val acc=0.81\n",
      "GAT training: time elapsed= 139.47 [s] | epoch=1701 | val acc=0.79\n",
      "GAT training: time elapsed= 147.69 [s] | epoch=1801 | val acc=0.798\n",
      "GAT training: time elapsed= 156.08 [s] | epoch=1901 | val acc=0.802\n",
      "GAT training: time elapsed= 164.46 [s] | epoch=2001 | val acc=0.8\n",
      "GAT training: time elapsed= 172.86 [s] | epoch=2101 | val acc=0.796\n",
      "GAT training: time elapsed= 181.26 [s] | epoch=2201 | val acc=0.804\n",
      "Stopping the training, the universe has no more patience for this training.\n",
      "Test accuracy = 0.826\n"
     ]
    }
   ],
   "source": [
    "train_gat(get_training_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f47f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
